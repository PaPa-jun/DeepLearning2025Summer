{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d223f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Parameter\n",
    "from torch.functional import F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import BPE, WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62a4f5",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57510327",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('enron_spam_data.csv').drop(columns=['Date']).rename(\n",
    "    columns={\n",
    "        'Message ID': 'id',\n",
    "        'Subject': 'abstract',\n",
    "        'Message': 'content',\n",
    "        'Spam/Ham': 'label',\n",
    "    }\n",
    ").set_index('id')\n",
    "data_frame.dropna(how='any', inplace=True)\n",
    "data_frame['label'] = data_frame['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "texts = data_frame[\"abstract\"].to_list() + data_frame[\"content\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62868f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer.encode(text).ids\n",
    "        encoding = encoding[:self.max_length] if len(encoding) > self.max_length \\\n",
    "            else encoding + [self.tokenizer.token_to_id(\"[PAD]\")] * (self.max_length - len(encoding))\n",
    "        return torch.tensor(encoding), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05489e",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c67e96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFKC(),\n",
    "    normalizers.Lowercase()\n",
    "])\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "trainer = WordLevelTrainer(\n",
    "    vocab_size = 30000,\n",
    "    min_frequency=20,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\"],\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd7d973a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17387"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save(\"tokenizer.json\")\n",
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456b72d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "728c29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    pass\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int = None):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.weight = Parameter(\n",
    "            torch.zeros(num_embeddings, embedding_dim),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        \n",
    "        # 处理padding_idx\n",
    "        if padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight[padding_idx].fill_(0)\n",
    "            self.weight[padding_idx].requires_grad = False\n",
    "\n",
    "    def forward(self, input: torch.LongTensor) -> torch.Tensor:\n",
    "        input = input.to(self.weight.device)\n",
    "        return self.weight[input]\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1fa05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "class WildeRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=128, embedding_dim = 256, output_size=2):\n",
    "        super(WildeRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.W_xh = Parameter(torch.randn(size=(embedding_dim, hidden_size)), requires_grad=True)\n",
    "        self.W_hh = Parameter(torch.randn(size=(hidden_size, hidden_size)), requires_grad=True)\n",
    "        self.b_h = Parameter(torch.zeros(hidden_size), requires_grad=True)\n",
    "\n",
    "        self.W_hq = Parameter(torch.randn(hidden_size, output_size), requires_grad=True)\n",
    "        self.b_q = Parameter(torch.zeros(output_size), requires_grad=True)\n",
    "\n",
    "        init.xavier_uniform_(self.W_xh)\n",
    "        init.orthogonal_(self.W_hh)\n",
    "        init.xavier_uniform_(self.W_hq)\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        embeded = self.embedding(inputs)\n",
    "        batch_size, seq_length, _ = embeded.shape\n",
    "        H = torch.zeros((batch_size, self.hidden_size)).to(device=embeded.device)\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            X = embeded[:, t, :]\n",
    "            H = torch.tanh(torch.mm(X, self.W_xh) + torch.mm(H, self.W_hh) + self.b_h)\n",
    "\n",
    "        return F.sigmoid(torch.mm(F.dropout(H, 0.3), self.W_hq) + self.b_q)\n",
    "    \n",
    "class BetterRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Using Pytorch RNN Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size=64, embedding_dim = 64, output_size=2):\n",
    "        super(BetterRNN, self).__init__()\n",
    "\n",
    "        self.rnn_layer = nn.Sequential(\n",
    "            nn.Embedding(vocab_size, embedding_dim),\n",
    "            nn.RNN(embedding_dim, hidden_size, batch_first=True, num_layers=1),\n",
    "        )\n",
    "\n",
    "        self.linear_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "       rnn_outputs, _ = self.rnn_layer(inputs)\n",
    "       return self.linear_layer(rnn_outputs[:, -1, :])\n",
    "\n",
    "# GRU\n",
    "class GRU(nn.Module):\n",
    "    pass\n",
    "\n",
    "# LSTM\n",
    "class LSTM(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155efd1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4385e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constract Training data.\n",
    "labels = data_frame[\"label\"].to_list() + data_frame[\"label\"].to_list()\n",
    "\n",
    "train_mail, val_mail, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "train_dataset = SpamDataset(train_mail, train_labels, tokenizer)\n",
    "val_dataset = SpamDataset(val_mail, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef1878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, lr, epochs, train_loader, val_loader, verbose=True, device=\"cpu\", save_path=\"best_model.pth\"):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predicts = (outputs > 0.5).float()\n",
    "            train_correct += (predicts == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.float().unsqueeze(1).to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicts = (outputs > 0.5).float()\n",
    "                val_correct += (predicts == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}. Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            if verbose:\n",
    "                print(f\"Best model saved at epoch {epoch+1} with Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss', marker='x')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"训练完成！最佳模型已保存为:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a99f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: Train Loss: 0.6814, Train Acc: 0.5112. Val Loss: 0.7213, Val Acc: 0.4959\n",
      "Best model saved at epoch 1 with Val Loss: 0.7213\n"
     ]
    }
   ],
   "source": [
    "rnn = BetterRNN(vocab_size=tokenizer.get_vocab_size(), output_size=1)\n",
    "train(rnn, 0.001, 10, train_loader, val_loader, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

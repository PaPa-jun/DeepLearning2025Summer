{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d223f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Parameter\n",
    "from torch.functional import F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62a4f5",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57510327",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv('enron_spam_data.csv').drop(columns=['Date']).rename(\n",
    "    columns={\n",
    "        'Message ID': 'id',\n",
    "        'Subject': 'abstract',\n",
    "        'Message': 'content',\n",
    "        'Spam/Ham': 'label',\n",
    "    }\n",
    ").set_index('id')\n",
    "data_frame.dropna(how='any', inplace=True)\n",
    "data_frame['label'] = data_frame['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "texts = data_frame[\"content\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05489e",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import SpamDataset, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(texts, min_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456b72d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728c29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    pass\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int = None):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.weight = Parameter(\n",
    "            torch.zeros(num_embeddings, embedding_dim),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        if padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight[padding_idx].fill_(0)\n",
    "            self.weight[padding_idx].requires_grad = False\n",
    "\n",
    "    def forward(self, input: torch.LongTensor) -> torch.Tensor:\n",
    "        input = input.to(self.weight.device)\n",
    "        return self.weight[input]\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=128, embedding_dim = 256, output_size=2):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.W_xh = Parameter(torch.randn(size=(embedding_dim, hidden_size)), requires_grad=True)\n",
    "        self.W_hh = Parameter(torch.randn(size=(hidden_size, hidden_size)), requires_grad=True)\n",
    "        self.b_h = Parameter(torch.zeros(hidden_size), requires_grad=True)\n",
    "\n",
    "        self.W_hq = Parameter(torch.randn(hidden_size, output_size), requires_grad=True)\n",
    "        self.b_q = Parameter(torch.zeros(output_size), requires_grad=True)\n",
    "\n",
    "        init.xavier_uniform_(self.W_xh)\n",
    "        init.orthogonal_(self.W_hh)\n",
    "        init.xavier_uniform_(self.W_hq)\n",
    "    \n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        embeded = self.embedding(inputs)\n",
    "        batch_size, seq_length, _ = embeded.shape\n",
    "        H = torch.zeros((batch_size, self.hidden_size)).to(device=embeded.device)\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            X = embeded[:, t, :]\n",
    "            H = torch.tanh(torch.mm(X, self.W_xh) + torch.mm(H, self.W_hh) + self.b_h)\n",
    "\n",
    "        return F.sigmoid(torch.mm(F.dropout(H, 0.3), self.W_hq) + self.b_q)\n",
    "\n",
    "# GRU\n",
    "class GRU(nn.Module):\n",
    "    pass\n",
    "\n",
    "# LSTM\n",
    "class LSTM(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2155efd1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4385e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constract Training data.\n",
    "labels = data_frame[\"label\"].to_list()\n",
    "\n",
    "train_mail, val_mail, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "train_dataset = SpamDataset(train_mail, train_labels, tokenizer)\n",
    "val_dataset = SpamDataset(val_mail, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses_plot(epochs, train_losses, val_losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss', marker='x')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "def train(model, lr, epochs, train_loader, val_loader, verbose=True, device=\"cpu\", save_path=\"best_model.pth\", save=False):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            predicts = (outputs > 0.5).float()\n",
    "            train_correct += (predicts == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.float().unsqueeze(1).to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predicts = (outputs > 0.5).float()\n",
    "                val_correct += (predicts == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_acc = train_correct / train_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}. Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        best_val_loss = avg_val_loss if avg_val_loss < best_val_loss else best_val_loss\n",
    "        if save:\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Best model saved at epoch {epoch+1} with Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    losses_plot(epochs, train_losses, val_losses)\n",
    "\n",
    "    print(\"训练完成！最佳模型已保存为:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(tokenizer.vocab_size, output_size=1)\n",
    "\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "train(rnn, 0.001, 10, train_loader, val_loader, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
